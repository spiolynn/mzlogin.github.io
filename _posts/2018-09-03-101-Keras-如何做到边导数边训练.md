---
layout:     post
title:      101-Keras-如何做到边导数边训练
subtitle:    "\"101-Keras-如何做到边导数边训练\""
date:       2018-09-03
author:     PZ
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - keras
    - AI
    - DL
---

```
2018-09-03-101-Keras-如何做到边导数边训练.md
```

## 101-Keras-如何做到边导数边训练

### 1 问题

> 在对Keras框架的学习中，一个很大的难点就是数据的导入，尤其是当数据不能一次放入内存的时候，应该如何导入的问题。



### 2 如何使用`ImageDataGenerator`和`datagen.flow `

### 2 fit_generator函数

`fit_generator(self, generator, steps_per_epoch, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_q_size=10, workers=1, pickle_safe=False, initial_epoch=0)
`
> 例如
```
from keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')
    
model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size,
    callbacks=[TensorBoard(log_dir='./log_dir')])
```

> 在Keras里有一个`fit_generator`函数

- generator：生成器函数，生成器的输出应该为：
    - 一个形如（inputs，targets）的tuple
    - 一个形如（inputs, targets,sample_weight）的tuple。所有的返回值都应该包含相同数目的样本。生成器将无限在数据集上循环。每个epoch以经过模型的样本数达到samples_per_epoch时，记一个epoch结束
- steps_per_epoch：整数，当生成器返回steps_per_epoch次数据时计一个epoch结束，执行下一个epoch
- epochs：整数，数据迭代的轮数
- validation_data 具有以下三种形式之一
    - 生成验证集的生成器
    - 一个形如（inputs,targets）的tuple
    - 一个形如（inputs,targets，sample_weights）的tuple
- validation_steps: 当validation_data为生成器时，本参数指定验证集的生成器返回次数
- workers：最大进程数



### 3 将图像数据通过vgg域训练模型，保持为特征提取数据

```python
'''
将图片数据，生成为npy数据 yield方式
'''

from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
import numpy as np
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input

datagen = ImageDataGenerator()

model = VGG16(weights='imagenet', include_top=False)

i = 1
for batch,label in datagen.flow_from_directory('data/train',
                                               target_size=(224, 224),
                                               batch_size=1,
                                               class_mode='binary',
                                               shuffle=False,
                                               classes=['dog','cat']):
    print(type(batch),batch.shape)
    print(type(label), label.shape)
    print('label' + str(label[0]))
    ## 生成npy文件

    # x = np.expand_dims(batch, axis=0)
    x = preprocess_input(batch)
    features = model.predict(x)
    print(features.shape)

    filename = r'data_np//train//' + str(i) + '_' +str(int(label[0]))+'.npz'
    print(filename)
    np.savez(filename,features=features,label=label)
    i += 1
    if i > 2:
        break
```

- 保存数据如下

![image.png](https://upload-images.jianshu.io/upload_images/10357485-2a645002ae7b00ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



### 4 完成一个generate生成器

- `npy_generate.py`

```python
import os
import numpy as np

def get_path_list(rootdir):
    '''
    :param rootdir: path 图片路径
    :return: file list
    '''
    FilePathList = []
    for fpathe, dirs, fs in os.walk(rootdir):
        for f in fs:
            FilePath = os.path.join(fpathe, f)
            if os.path.isfile(FilePath):
                FilePathList.append(FilePath)
    return FilePathList

def get_npy_data_by_batch(FilePathList,batch_size):
    '''
    :param FilePathList: FILElIST
    :param batch_size:   小批数量
    :return: (batch_size,7,7,12) (batch_size,1)
    '''
    # FilePathList = get_path_list(x_path)
    FilePathList_Len = len(FilePathList)
    # print(FilePathList_Len)
    FileIndexs = np.random.randint(0, FilePathList_Len, batch_size)
    # print(FileIndexs)

    i = 0
    for file_i in FileIndexs:
        npz = np.load(FilePathList[file_i])
        features = npz['features']
        label = npz['label']
        if i == 0:
            features_np_list = features
            lable_np_list = label
        else:
            features_np_list = np.vstack((features_np_list,features))
            lable_np_list = np.vstack((lable_np_list,label))
        i = i + 1
    return features_np_list,lable_np_list




def generate_batch_data_random(x_path,batch_size):
    """逐步提取batch数据到显存，降低对显存的占用"""

    FilePathList = get_path_list(x_path)
    while (True):
        features_np_list, lable_np_list = get_npy_data_by_batch(FilePathList, batch_size)
        (train_x,train_y) = features_np_list, lable_np_list
        yield (train_x,train_y)


if __name__ == '__main__':
    i = 1
    for data in generate_batch_data_random('data_np',2):
        print(type(data))
        i = i + 1
        if i>5:
            break
```

### 6 vgg-training

- `keras_vgg.py`

```
from __future__ import print_function
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
import numpy as np
from keras.callbacks import TensorBoard
from keras.applications.vgg16 import VGG16
from PIL import Image
from keras.callbacks import TensorBoard

import sys
import os
sys.path.append(os.getcwd())
print(os.getcwd())

from npy_training import generate_batch_data_random

# step1- 数据
data_path = 'data_np/train'
valid_path = 'data_np/validation'
batch_size = 8

# step2- 模型设计
model = VGG16(weights='imagenet', include_top=False)
model = Sequential()
model.add(Flatten(input_shape=(7,7,512)))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit_generator(
        generate_batch_data_random(data_path,batch_size),
        samples_per_epoch=2000//batch_size,
        nb_epoch=20,
        validation_data=generate_batch_data_random(valid_path,batch_size),
        nb_val_samples=400//batch_size,
        callbacks = [TensorBoard(log_dir='./log_dir')]
        )

model.save_weights('VGG_MNIST.h5')
```

### 7 一个问题

> 如下是batch_size=128情况下的训练情况

```
Epoch 1/20
15/15 [==============================] - 7s 493ms/step - loss: 2.1259 - acc: 0.8516 - val_loss: 0.9224 - val_acc: 0.9349
Epoch 2/20
15/15 [==============================] - 5s 365ms/step - loss: 0.8413 - acc: 0.9448 - val_loss: 0.4484 - val_acc: 0.9661
Epoch 3/20
15/15 [==============================] - 5s 352ms/step - loss: 0.9262 - acc: 0.9380 - val_loss: 2.3249 - val_acc: 0.8542
Epoch 4/20
15/15 [==============================] - 6s 374ms/step - loss: 0.7324 - acc: 0.9526 - val_loss: 0.6391 - val_acc: 0.9557
Epoch 5/20
15/15 [==============================] - 5s 354ms/step - loss: 0.5139 - acc: 0.9667 - val_loss: 1.6390 - val_acc: 0.8958
Epoch 6/20
15/15 [==============================] - 5s 356ms/step - loss: 0.5754 - acc: 0.9630 - val_loss: 0.5367 - val_acc: 0.9661
Epoch 7/20
15/15 [==============================] - 6s 375ms/step - loss: 0.5179 - acc: 0.9646 - val_loss: 0.4235 - val_acc: 0.9714
Epoch 8/20
15/15 [==============================] - 5s 358ms/step - loss: 0.5342 - acc: 0.9630 - val_loss: 0.4364 - val_acc: 0.9609
Epoch 9/20
15/15 [==============================] - 6s 372ms/step - loss: 0.3254 - acc: 0.9792 - val_loss: 0.2303 - val_acc: 0.9818
Epoch 20/20
15/15 [==============================] - 6s 368ms/step - loss: 0.2896 - acc: 0.9802 - val_loss: 0.3358 - val_acc: 0.9792
```

> 如下是batch_size=8情况下的训练情况

```
Epoch 1/20
250/250 [==============================] - 24s 98ms/step - loss: 8.0961 - acc: 0.4910 - val_loss: 8.3698 - val_acc: 0.4750
Epoch 2/20
250/250 [==============================] - 25s 99ms/step - loss: 8.0350 - acc: 0.4960 - val_loss: 7.6922 - val_acc: 0.5175
Epoch 3/20
250/250 [==============================] - 24s 97ms/step - loss: 5.2238 - acc: 0.6700 - val_loss: 1.8216 - val_acc: 0.8775
Epoch 4/20
250/250 [==============================] - 24s 98ms/step - loss: 1.0436 - acc: 0.9305 - val_loss: 0.7948 - val_acc: 0.9500
```

- 多次测试发现，batch_size 如果设置过小，大概率可能导致模型无法收敛


### 3 参考

> https://www.jianshu.com/p/0fbe5b5d0ab8 <br> 在Keras中导入测试数据的方法

> https://blog.csdn.net/sinat_26917383/article/details/74922230 <br> 利用fit_generator最小化显存占用比率/数据Batch化

> https://liam0205.me/2017/06/30/understanding-yield-in-python/ <br> 什么是yield

