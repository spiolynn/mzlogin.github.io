---
layout:     post
title:      002-中文词嵌入-数据集-模型
subtitle:    "\"NLP-中文词嵌入-数据集-模型\""
date:       2018-10-30
author:     PZ
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - nlp
    - 词嵌入
---

## 002-中文词嵌入-数据集-模型

> https://kexue.fm/archives/4515 <br> keras版Word2vec

> https://eliyar.biz/using-pre-trained-gensim-word2vector-in-a-keras-model-and-visualizing/ <br> Keras 模型中使用预训练的 gensim 词向量和可视化

> https://github.com/zake7749/word2vec-tutorial <br> github 中文词嵌入

> https://github.com/AimeeLee77/wiki_zh_word2vec <br> 利用Python构建Wiki中文语料词向量模型试验 - **使用**

> https://github.com/AimeeLee77/keyword_extraction <br> 利用Python实现中文文本关键词抽取，分别采用TF-IDF、TextRank、Word2Vec词聚类三种方法

> https://github.com/cjymz886/text-cnn <br> 嵌入Word2vec词向量的CNN中文文本分类

### 1 完成一个基于中文语料的词向量模型构建

#### 1 准备

> 1 数据：（wiki中文语料1.7G）https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2

> 2 环境：python3 pip install gensim

> 3 openCC (https://bintray.com/package/files/byvoid/opencc/OpenCC) 繁体-to-简体

#### 2 数据预处理

##### 2.1 将XML的Wiki数据转换为text格式

```
import logging
import os.path
import sys

from gensim.corpora import WikiCorpus

if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])#得到文件名
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info("running %s" % ' '.join(sys.argv))

    inp = r'C:\Users\hupan\Downloads\zhwiki-latest-pages-articles.xml.bz2'
    outp = 'wiki.txt'
    space = " "
    i = 0

    output = open(outp, 'w',encoding='utf-8')
    wiki =WikiCorpus(inp, lemmatize=False, dictionary=[])
    #gensim里的维基百科处理类WikiCorpus
    for text in wiki.get_texts():
        #通过get_texts将维基里的每篇文章转换位1行text文本，并且去掉了标点符号等内容
        output.write(space.join(text) + "\n")
        i = i+1
        if (i % 10000 == 0):
            logger.info("Saved "+str(i)+" articles.")

    output.close()
    logger.info("Finished Saved "+str(i)+" articles.")
```    

![image.png](https://upload-images.jianshu.io/upload_images/14744153-39b740b9504052f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

##### 2.2 中文繁体替换成简体

```
./opencc.exe -i C:\\004_project\\009-NLP\\002_word_classify\\word_vec\\wiki.txt -o C:\\004_project\\009-NLP\\002_word_classify\\word_vec\\wiki.zh.simp.txt -c t2s.json
```

> 下载地址：<br> https://bintray.com/package/files/byvoid/opencc/OpenCC <br> 解压直接使用

![image.png](https://upload-images.jianshu.io/upload_images/14744153-80432d4b645c4a23.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

##### 2.3 分词

> 由于此语料已经去除了标点符号，因此在分词程序中无需进行清洗操作，可直接分词。若是自己采集的数据还需进行标点符号去除和去除停用词的操作。 

```python
import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs,sys


if __name__ == '__main__':
    f = codecs.open('wiki.zh.simp.txt', 'r', encoding='utf8')
    target = codecs.open('wiki.zh.simp.seg.txt', 'w', encoding='utf8')
    print ('open files.')

    lineNum = 1
    line = f.readline()
    while line:
        print ('---processing ',lineNum,' article---')
        seg_list = jieba.cut(line,cut_all=False)
        line_seg = ' '.join(seg_list)
        target.writelines(line_seg)
        lineNum = lineNum + 1
        line = f.readline()

    print ('well done.')
    f.close()
    target.close()
```

#### 3 Word2Vec模型训练

```python
# 使用gensim word2vec训练脚本获取词向量

import warnings

warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')  # 忽略警告

import logging
import os.path
import sys
import multiprocessing

from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

if __name__ == '__main__':
    # print open('/Users/sy/Desktop/pyRoot/wiki_zh_vec/cmd.txt').readlines()
    # sys.exit()

    program = os.path.basename('log.log')
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)
    logger.info("running %s" % ' '.join(sys.argv))

    # inp为输入语料, outp1为输出模型, outp2为原始c版本word2vec的vector格式的模型
    fdir = './'
    inp = fdir + 'wiki.zh.simp.seg.txt'
    outp1 = fdir + 'wiki.zh.text.model'
    outp2 = fdir + 'wiki.zh.text.vector'

    # 训练skip-gram模型
    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5,
                     workers=multiprocessing.cpu_count())

    # 保存模型
    model.save(outp1)
    model.wv.save_word2vec_format(outp2, binary=False)
```

> 训练完成：`wiki.zh.text.model`模型  `wiki.zh.text.vector`词向量。

#### 4 模型测试

```python
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')# 忽略警告
import sys
# sys.setdefaultencoding('utf8')
import gensim


if __name__ == '__main__':
    fdir = './'
    model = gensim.models.Word2Vec.load(fdir + 'wiki.zh.text.model')

    # 某一个词向量最相近的词集合
    word = model.most_similar(u"程序员",topn=20)
    for t in word:
        print(t[0],t[1])

    word = model.most_similar(positive=[u'皇上',u'国王'],negative=[u'皇后'])
    for t in word:
        print (t[0],t[1])

    # 两个词向量的相近程度
    print(model.similarity(u'书籍', u'书本'))
    print(model.similarity(u'逛街', u'书本'))

    # 区分词含义
    print (model.doesnt_match(u'太后 妃子 贵人 贵妃 才人'.split()))

```

```
开发人员 0.7226051092147827
开发者 0.6633485555648804
编程 0.6427068114280701
电脑程式 0.6324516534805298
编译器 0.624819278717041
脚本语言 0.6080538034439087
程式设计 0.6027412414550781
编程语言 0.5947513580322266
应用程序 0.5905519723892212
开发工具 0.5809053778648376
臣子 0.48980608582496643
先王 0.46897774934768677
圣上 0.453904390335083
天下人 0.45261895656585693
王命 0.44982022047042847
廷臣 0.4481680989265442
岂敢 0.4422057271003723
子产 0.4413231313228607
三世 0.4392887353897095
叔向 0.43806880712509155
太后
0.6192949
0.15757455
```