---
layout:     post
title:      008-Keras-找到最好的模型
subtitle:    "\"Keras-找到最好的模型\""
date:       2018-08-28
author:     PZ
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - keras
    - AI
    - DL
---

```
2018-08-28-008-Keras-找到最好的模型.md
```

## 008-Keras-找到最好的模型

### 1 checkpoint

> 在Deep Learning过程中，常常是一个耗时，漫长的过程，过程存在着`停电` `OS down 掉` `需要中断`等实际情况，如果training 只在完成时，保持模型，当遇到上诉问题时，之前的计算耗时就白白浪费了，所以`checkpoint` 功能就变得很重要。他就像是打游戏的存档点，不让中间过程白白浪费。

#### 1.1 Keras' CheckPoint

- 模型的体系结构，允许你重新创建模型
- 模型的权重
- 训练配置(损失、优化器、epochs和其他元信息)
- 优化器的状态，允许在你离开的地方恢复训练

#### 1.2 CheckPoint 策略 

- 短期训练制度(几分钟到几小时)

> 典型的做法是在训练结束时，或者在每个epoch结束时，保存一个检查点。

- 正常的训练制度(数小时到一整天)

> 在这种情况下，在每个`n_epochs`中保存多个检查点，并跟踪我们所关心的一些验证度量，这是很常见的。通常，有一个固定的最大数量的检查点，这样就不会占用太多的磁盘空间(例如，将你最大的检查点数量限制在10个，新的位置将会取代最早的检查点)。
    
- 长期训练制度(数天至数周)

> 在这种类型的训练体系中，你可能希望采用与常规机制类似的策略：在每一个`n_epochs`中，你都可以节省多个检查点，并在你所关心的验证度量上保持最佳状态。在这种情况下，由于训练将花费很长的时间，所以减少检查点的次数是很常见的，但是需要维护更多的检查点。

### 2 checkpoint-how to use (Keras)

> https://www.floydhub.com/redeipirati/projects/save-and-resume/53/code/keras_mnist_cnn_jupyter.ipynb <br> 栗子

#### 2.1 保存`检查点`

```
# keras learning
from keras.layers import Input, Dense
from keras.models import Model
import numpy as np
from keras.callbacks import TensorBoard
from keras.callbacks import ModelCheckpoint

# filepath="weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5"
filepath = "mnist-cnn-best.hdf5"

'''
monitor=’val_acc’:这是我们所关心的度量：验证精确度
verbose=1:它将打印更多信息
save_best_only=True:只保留最好的检查点(在最大化验证精确度的情况下)
mode=’max’:以最大化验证精确度保存检查点
'''
checkpoint = ModelCheckpoint(filepath,
 monitor='val_acc',
 verbose=1,
 save_best_only=True,
 mode='max',
 period=1)



from keras.layers import Input, Dense
from keras.models import Model
import keras

# 生成虚拟数据
import numpy as np
data = np.random.random((1000, 784))
## 生成0-9 随机数 (1000,1) 1000维列向量
labels = np.random.randint(10, size=(1000, 1))
# 将标签转换为分类的 one-hot 编码
one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)

# 这部分返回一个张量
inputs = Input(shape=(784,))

# 层的实例是可调用的，它以张量为参数，并且返回一个张量
x = Dense(64, activation='relu')(inputs)
x = Dense(64, activation='relu')(x)
predictions = Dense(10, activation='softmax')(x)

# 这部分创建了一个包含输入层和三个全连接层的模型
model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])



model.fit(data, one_hot_labels, validation_data=(data, one_hot_labels),epochs=10, callbacks=[TensorBoard(log_dir='./log_dir'),checkpoint])  # 开始训练

score = model.evaluate(data, one_hot_labels, verbose=0)
################################## model

################################## tensorboard
```

```
-rw-r--r--  1 root root 463168 Aug 29 10:53 weights-improvement-01-0.10.hdf5
-rw-r--r--  1 root root 463168 Aug 29 10:53 weights-improvement-02-0.13.hdf5
-rw-r--r--  1 root root 463168 Aug 29 10:53 weights-improvement-04-0.15.hdf5
-rw-r--r--  1 root root 463168 Aug 29 10:53 weights-improvement-06-0.18.hdf5
-rw-r--r--  1 root root 463168 Aug 29 10:53 weights-improvement-07-0.20.hdf5
-rw-r--r--  1 root root 463168 Aug 29 10:53 weights-improvement-09-0.21.hdf5
```

> 注意 这里保持的是model的weights，读档如下：

```
model.load_weights("weights.best.hdf5")
# Compile model (required to make predictions) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print("Created model and loaded weights from file")
# load pima indians dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# estimate accuracy on whole dataset using loaded weights
scores = model.evaluate(X, Y, verbose=0)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
```




### 3 参考

> https://cnbeining.github.io/deep-learning-with-python-cn/4-advanced-multi-layer-perceptrons-and-keras/ch14-keep-the-best-models-during-training-with-checkpointing.html

> http://www.atyun.com/12192.html

> https://www.floydhub.com/redeipirati/projects/save-and-resume/53/code/keras_mnist_cnn_jupyter.ipynb