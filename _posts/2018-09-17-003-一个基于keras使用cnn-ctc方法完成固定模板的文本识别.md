---
layout:     post
title:      003-一个基于keras使用cnn-ctc方法完成固定模板的文本识别
subtitle:    "\"一个基于keras使用cnn-ctc方法完成固定模板的文本识别\""
date:       2018-09-17
author:     PZ
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - keras
    - DL
    - AI
    - OCR
    - 文本定位
---

## 003-基于keras框架使用cnn-ctc方法完成模板类型文本识别

> 在实际工作中，有很多模板类文本识别的场景，比如、身份证、驾驶证等等证照类的识别、在比如特定格式图片文本识别。在这些场景中，根据场景类型、识别难度、可以做一个简单的分类

- 场景分类
    - 特定模板类
    - 自然场景类
- 文字类型
    - 打印文本
    - 手写文本
- 文字范围
    - 封闭集（如数字类）（特定文字类）
    - 开放集

> 最简单场景：特定模板的打印体封闭集识别 (验证码识别)（身份证中省份证号识别）...

> 最复杂场景：自然场景下手写体开放集识别 

本着先易后难，我们可以先完成一个最简单的场景。**特定模板的打印体封闭集识别**。

### 1 方法

#### 1.1 object detection 方法

在计算机视觉中的目标检测中，有两个方法流派，

- 1 两阶段（2-stage）检测

> 两阶段检测也叫基于区域（Region-based）的方法，简单来说，就是第一阶段，完成对象定位，第二阶段进行对象识别。 放在文本识别场景中，就行先进行文本定位，在进行文本识别。

> 2-stage 开山之作：就是RBG的RCNN家族算法（fast-rcnn faster-rcnn）最近两年，何凯明的Mask R-CNN 也属于2-stage ，在图像文本领域，CPTN是在行文本定位，使用最广泛的算法。

- 2 单阶段检测

> 单阶段检测，顾名思义，就是将定位，识别放在一个模型中一次完成。

> 1-stage 开山之作2016年Joseph的YOLO方法，后续 SSD 算是这类中教成熟的算法。 在文本识别中将文本定位基于SSD算法是一种比较常见的方法，如`SSTD`。

#### 1.2 文本object detection 方法

如上所述，文本图片的识别，一般都会经过`文本定位` `文本识别`这样2-stage 的过程。

##### 先说文本定位：

> 下图是旷视科技在2018年CVPR（全球计算机视觉顶会）中，提出新算法，在水平行文本定位下的评价指标的打分。`ours` 分自然表现不错，`CPTN` `SSTD` 的表现也是十分出色的。

![image.png](https://upload-images.jianshu.io/upload_images/10357485-2f2f2e67508ea02f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> https://zhuanlan.zhihu.com/p/37016307 <br> 这里可以了解更多旷视论文的内容细节

前面说的都是，基于Deep learning 的方法，当然也别忘了，还有传统一点，

- 图像处理+机器学习（MSER+SVM(adaboost)） 或者 
- 模板匹配(SIFT SURF BRISK)的方法。


##### 再说文本识别：

文本定位完成后，获取到文本图片切片，后续就是文本识别的工作了。
- 如果场景简单，比如文本是固定长度的，文本的集合是在一个可控范围，CNN+softmax多分类，是一种可行的方法。
- 如果场景复杂，文本长度不固定，CNN+RNN(可选)+CTC 是一种普遍的算法讨论。


### 3 开工

> 好了，上面的背书说完，回到主题，这里要完成一个固定模板的文本识别，这里的固定模板可以是`验证码` `身份证` 或者其他的模板。

这里在文本定位阶段，我已省份证为例，文本识别阶段，我已其他模板的识别为例。


#### 3.1 身份证的定位问题

> sift 特征识别方法定位

在 `001-图像处理-完成一个简单的以图搜图功能` 中已经说明了如何使用`sift`算子完成，包含身份证图片的搜索和定位，后续其实只需要将图片进行校正和切割就可以完成定位的工作。这里不废话，文后有简单的代码可以参考。

![image.png](https://upload-images.jianshu.io/upload_images/10357485-d91ff333617f219f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```python
'''
基于模板方式的文本定位
function version
'''

import numpy as np
import cv2
from matplotlib import pyplot as plt
from imutils.perspective import four_point_transform
import os
import shutil

def sift_compare_pic(img1,img2,MIN_MATCH_COUNT,threshold,img3):
    '''
    :param pic_1:  img1模板
    :param pic_2:  img2待搜索图片
    :MIN_MATCH_COUNT : 匹配阈值
    :return:       Ture 是 False 否
    '''

    try:
        img1_shape = img1.shape
        # print(img1_shape) (82, 1044)
        # Initiate SIFT detector
        sift = cv2.xfeatures2d.SIFT_create()

        kp1, des1 = sift.detectAndCompute(img1,None)
        kp2, des2 = sift.detectAndCompute(img2,None)

        FLANN_INDEX_KDTREE = 0
        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
        search_params = dict(checks = 50)

        flann = cv2.FlannBasedMatcher(index_params, search_params)

        matches = flann.knnMatch(des1,des2,k=2)

        # store all the good matches as per Lowe's ratio test.
        good = []
        for m,n in matches:
            if m.distance < threshold*n.distance:
                good.append(m)

        if len(good) > MIN_MATCH_COUNT:
            src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
            dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
            matchesMask = mask.ravel().tolist()

            h, w = img1.shape
            pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)
            dst = cv2.perspectiveTransform(pts, M)
            print(dst)
            print(dst.shape)
            # img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)
            img2 = cv2.polylines(img2, [np.int32(dst)], True, (0,255,255))
            warped = four_point_transform(img3, np.int32(dst).reshape(4, 2))
            #plt.imshow(warped, 'gray'), plt.show()
            warped = cv2.resize(warped, (img1_shape[1], img1_shape[0]))
            return warped
            # print(img1_shape)
            # cv2.imwrite('test.jpg', warped)
        else:
            return False
    except Exception as e:
        return False

def detection(img,rect):
    '''
    文本框定位
    :param img: img
    :param rect: (x,y,w.h)
    :return: img_rect
    '''
    (x,y,w,h) = rect
    # cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 3)
    roi = img[y:y+h,x:x+w]
    # plt.imshow(img), plt.show()
    return roi

if __name__ == '__main__':
    # img_mode_path = r'test_pic/000003.jpg'
    # img_path = r'test_pic/1234.jpg'
    # img_mode_path = r'test_pic/21.png'
    # img_path = r'test_pic/2.png'

    img_mode_path = r'test_pic/000003.jpg'
    img_path = r'test_pic/000875.jpg'
    img_mode = cv2.imread(img_mode_path, 0)
    plt.figure(1), plt.imshow(img_mode)
    img_gray = cv2.imread(img_path, 0)
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.figure(2), plt.imshow(img)
    target_img = sift_compare_pic(img_mode,img_gray,10,0.8,img)
    # plt.imshow(target_img), plt.show()

    ############ 定位 82, 1044
    # roi = detection(target_img,(735,22,305,59))
    roi = detection(target_img, (309, 192, 650, 100))
    plt.figure(3),plt.imshow(roi), plt.show()
```

> ctpn 方式定位

ctpn的定位方法，更为通用，当然也更新复杂，一些，他需要对`fast-rcnn`原理有一定了解。所以，这部分内容，后续，需要单独时间来说明这个方法的实现。在github在已经有对应是实现，可以了解一下，`https://github.com/spiolynn/OCR-end-to-end` , 网络上，也有大量对该方法的讲述。


#### 3.2 封闭集合的文本识别

> 前文中说道，在文本识别中，简单场景（封闭集）可使用 cnn+softmax 多分类的方法， 这里提供一篇博文`https://ypw.io/captcha/` 完成的是`固定长度的验证码`识别。

> tip: 博文中，也实现了cnn+ctc的方法，但是笔者亲测，模型无法收敛，而且代码中，存在几处误导。

所以这里，我使用的是，具体模板的下，数字图片的识别（封闭集：只包含数字）
（版权问题，图片不做展示）

在`GPU 1050 + 10000张图片（9:1）`数据量下，training到第2轮，即可收敛到，90%以上准确率,下图是 1000张测试集，图片的准确率91.25%

> https://github.com/spiolynn/CNN_CTC-.git <br> 详细代码见github

```
str_src=118295678  str_out=118295678
str_src=213696  str_out=213696
str_src=7000000  str_out=7000000
str_src=7000000  str_out=7000000
...
str_src=916000  str_out=916000
str_src=5599987  str_out=5599987
str_src=14663063  str_out=14663063
str_src=2160000  str_out=2160000
str_src=65550  str_out=65550
str_src=8884  str_out=8884
str_src=4034100  str_out=4034100
str_src=9484546  str_out=9484546
str_src=216000  str_out=216000
acc = 0.9125
```

#### 3.3 模型结构

模型采用 DenseNet + CTC 结构

```
-------------- model -------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 85, 350, 3)   0                                            
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 43, 175, 64)  4800        input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 43, 175, 64)  256         conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 43, 175, 64)  0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 43, 175, 8)   4616        activation_28[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 43, 175, 72)  0           conv2d_28[0][0]                  
                                                                 conv2d_29[0][0]                  
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 43, 175, 72)  288         concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 43, 175, 72)  0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 43, 175, 8)   5192        activation_29[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 43, 175, 80)  0           concatenate_25[0][0]             
                                                                 conv2d_30[0][0]                  
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 43, 175, 80)  320         concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 43, 175, 80)  0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 43, 175, 8)   5768        activation_30[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 43, 175, 88)  0           concatenate_26[0][0]             
                                                                 conv2d_31[0][0]                  
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 43, 175, 88)  352         concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 43, 175, 88)  0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 43, 175, 8)   6344        activation_31[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 43, 175, 96)  0           concatenate_27[0][0]             
                                                                 conv2d_32[0][0]                  
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 43, 175, 96)  384         concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 43, 175, 96)  0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 43, 175, 8)   6920        activation_32[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 43, 175, 104) 0           concatenate_28[0][0]             
                                                                 conv2d_33[0][0]                  
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 43, 175, 104) 416         concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 43, 175, 104) 0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 43, 175, 8)   7496        activation_33[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 43, 175, 112) 0           concatenate_29[0][0]             
                                                                 conv2d_34[0][0]                  
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 43, 175, 112) 448         concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 43, 175, 112) 0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 43, 175, 8)   8072        activation_34[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 43, 175, 120) 0           concatenate_30[0][0]             
                                                                 conv2d_35[0][0]                  
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 43, 175, 120) 480         concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 43, 175, 120) 0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 43, 175, 8)   8648        activation_35[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 43, 175, 128) 0           concatenate_31[0][0]             
                                                                 conv2d_36[0][0]                  
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 43, 175, 128) 512         concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 43, 175, 128) 0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 43, 175, 128) 16384       activation_36[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 43, 175, 128) 0           conv2d_37[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 21, 87, 128)  0           dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 21, 87, 128)  512         average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 21, 87, 128)  0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 21, 87, 8)    9224        activation_37[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 21, 87, 136)  0           average_pooling2d_3[0][0]        
                                                                 conv2d_38[0][0]                  
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 21, 87, 136)  544         concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 21, 87, 136)  0           batch_normalization_38[0][0]     
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 21, 87, 8)    9800        activation_38[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 21, 87, 144)  0           concatenate_33[0][0]             
                                                                 conv2d_39[0][0]                  
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 21, 87, 144)  576         concatenate_34[0][0]             
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 21, 87, 144)  0           batch_normalization_39[0][0]     
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 21, 87, 8)    10376       activation_39[0][0]              
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 21, 87, 152)  0           concatenate_34[0][0]             
                                                                 conv2d_40[0][0]                  
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 21, 87, 152)  608         concatenate_35[0][0]             
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 21, 87, 152)  0           batch_normalization_40[0][0]     
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 21, 87, 8)    10952       activation_40[0][0]              
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 21, 87, 160)  0           concatenate_35[0][0]             
                                                                 conv2d_41[0][0]                  
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 21, 87, 160)  640         concatenate_36[0][0]             
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 21, 87, 160)  0           batch_normalization_41[0][0]     
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 21, 87, 8)    11528       activation_41[0][0]              
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 21, 87, 168)  0           concatenate_36[0][0]             
                                                                 conv2d_42[0][0]                  
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 21, 87, 168)  672         concatenate_37[0][0]             
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 21, 87, 168)  0           batch_normalization_42[0][0]     
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 21, 87, 8)    12104       activation_42[0][0]              
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 21, 87, 176)  0           concatenate_37[0][0]             
                                                                 conv2d_43[0][0]                  
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 21, 87, 176)  704         concatenate_38[0][0]             
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 21, 87, 176)  0           batch_normalization_43[0][0]     
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 21, 87, 8)    12680       activation_43[0][0]              
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 21, 87, 184)  0           concatenate_38[0][0]             
                                                                 conv2d_44[0][0]                  
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 21, 87, 184)  736         concatenate_39[0][0]             
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 21, 87, 184)  0           batch_normalization_44[0][0]     
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 21, 87, 8)    13256       activation_44[0][0]              
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 21, 87, 192)  0           concatenate_39[0][0]             
                                                                 conv2d_45[0][0]                  
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 21, 87, 192)  768         concatenate_40[0][0]             
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 21, 87, 192)  0           batch_normalization_45[0][0]     
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 21, 87, 128)  24576       activation_45[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 21, 87, 128)  0           conv2d_46[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 10, 43, 128)  0           dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 10, 43, 128)  512         average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 10, 43, 128)  0           batch_normalization_46[0][0]     
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 10, 43, 8)    9224        activation_46[0][0]              
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 10, 43, 136)  0           average_pooling2d_4[0][0]        
                                                                 conv2d_47[0][0]                  
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 10, 43, 136)  544         concatenate_41[0][0]             
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 10, 43, 136)  0           batch_normalization_47[0][0]     
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 10, 43, 8)    9800        activation_47[0][0]              
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 10, 43, 144)  0           concatenate_41[0][0]             
                                                                 conv2d_48[0][0]                  
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 10, 43, 144)  576         concatenate_42[0][0]             
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 10, 43, 144)  0           batch_normalization_48[0][0]     
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 10, 43, 8)    10376       activation_48[0][0]              
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 10, 43, 152)  0           concatenate_42[0][0]             
                                                                 conv2d_49[0][0]                  
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 10, 43, 152)  608         concatenate_43[0][0]             
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 10, 43, 152)  0           batch_normalization_49[0][0]     
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 10, 43, 8)    10952       activation_49[0][0]              
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 10, 43, 160)  0           concatenate_43[0][0]             
                                                                 conv2d_50[0][0]                  
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 10, 43, 160)  640         concatenate_44[0][0]             
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 10, 43, 160)  0           batch_normalization_50[0][0]     
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 10, 43, 8)    11528       activation_50[0][0]              
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 10, 43, 168)  0           concatenate_44[0][0]             
                                                                 conv2d_51[0][0]                  
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 10, 43, 168)  672         concatenate_45[0][0]             
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 10, 43, 168)  0           batch_normalization_51[0][0]     
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 10, 43, 8)    12104       activation_51[0][0]              
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 10, 43, 176)  0           concatenate_45[0][0]             
                                                                 conv2d_52[0][0]                  
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 10, 43, 176)  704         concatenate_46[0][0]             
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 10, 43, 176)  0           batch_normalization_52[0][0]     
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 10, 43, 8)    12680       activation_52[0][0]              
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 10, 43, 184)  0           concatenate_46[0][0]             
                                                                 conv2d_53[0][0]                  
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 10, 43, 184)  736         concatenate_47[0][0]             
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 10, 43, 184)  0           batch_normalization_53[0][0]     
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 10, 43, 8)    13256       activation_53[0][0]              
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 10, 43, 192)  0           concatenate_47[0][0]             
                                                                 conv2d_54[0][0]                  
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 10, 43, 192)  768         concatenate_48[0][0]             
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 10, 43, 192)  0           batch_normalization_54[0][0]     
__________________________________________________________________________________________________
permute (Permute)               (None, 43, 10, 192)  0           activation_54[0][0]              
__________________________________________________________________________________________________
flatten (TimeDistributed)       (None, 43, 1920)     0           permute[0][0]                    
__________________________________________________________________________________________________
the_labels (InputLayer)         (None, 11)           0                                            
__________________________________________________________________________________________________
out (Dense)                     (None, 43, 11)       21131       flatten[0][0]                    
__________________________________________________________________________________________________
input_length (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
label_length (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
ctc (Lambda)                    (None, 1)            0           the_labels[0][0]                 
                                                                 out[0][0]                        
                                                                 input_length[0][0]               
                                                                 label_length[0][0]               
==================================================================================================
Total params: 314,763
Trainable params: 307,275
Non-trainable params: 7,488
__________________________________________________________________________________________________
```

#### 3.4 代码说明

- training_main.py 训练函数

```
def training_1():
    pass
    a_model = Model_DenseNet_CTC_HW() #模型定义
    data_train = DataLoad_Model_by_Path(train_path) #训练数据gen类实例化
    data_val = DataLoad_Model_by_Path(val_path)
    a_model.build_model()
    # a_model.load_model_weight('../saved_models-16092018-172058-e100.h5') # 基于已有weight训练
    a_model.train_generator(data_gen_train = data_train.gen_data_by_batch_ctc_yield_hw,
                            data_gen_val = data_val.gen_data_by_batch_ctc_yield_hw,
                            epochs=100,
                            batch_size=16,
                            steps_per_epoch=9000//16,
                            validation_steps = 10)
```

- Model_DenseNet_CTC_HW.py 模型定义类

```
    def build_model(self):
        '''
        :param height:
        :param width:
        :param n_class: 字符种类
        :param seq_len: 预测字符最大长度
        :return:
        '''
        # 模型层次构建
        # configs 使用配置文件方式构建
        timer = Timer()
        timer.start()
        print('[Model] Model begin')

        ########
        input = Input((height, width, 3)) # 这里height, width顺序不要颠倒
        pred_conv = densenet.dense_cnn(input, n_class)

        conv_shape = pred_conv.get_shape()
        self.conv_shape = conv_shape
        # x = Reshape(target_shape=(int(conv_shape[1]), int(conv_shape[2] * conv_shape[3])))(x)
        # pred_conv = Dense(n_class, activation='softmax')(x)
        self.basemodel = Model(inputs=input, outputs=pred_conv)

        print('-------------- base model -------------')
        print(self.basemodel.summary())

        # (*,11)
        labels = Input(name='the_labels', shape=[seq_len], dtype='float32')
        # (*,1)
        input_length = Input(name='input_length', shape=[1], dtype='int64')
        # (*,1)
        label_length = Input(name='label_length', shape=[1], dtype='int64')

        loss_out = Lambda(self.ctc_lambda_func, output_shape=(1,), name='ctc')([labels, pred_conv, input_length, label_length])
        self.model = Model(input=[labels, input, input_length, label_length], output=[loss_out])
        self.model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer='adam')

        print('-------------- model -------------')
        print(self.model.summary())
        #########

        print('[Model] Model Compiled')
        timer.stop()


   def train_generator(self, data_gen_train, data_gen_val,epochs, batch_size, steps_per_epoch,validation_steps):
        # 生成方式训练
        timer = Timer()
        timer.start()
        print('[Model] Training Started')
        print('[Model] %s epochs, %s batch size, %s batches per epoch' % (epochs, batch_size, steps_per_epoch))

        save_fname = 'saved_models-%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs))
        checkpoint = ModelCheckpoint(save_fname,
                                     monitor='val_loss',
                                     verbose=1,
                                     save_best_only=True,
                                     mode='min',
                                     period=1)
        print('=================')
        print(type(self))
        evaluator = Evaluate(self)

        lr_schedule = lambda epoch: 0.0005 * 0.4 ** epoch # 动态学习率调整很有效果
        learning_rate = np.array([lr_schedule(i) for i in range(10)])
        changelr = LearningRateScheduler(lambda epoch: float(learning_rate[epoch]))  # 动态学习率

        callbacks = [
            evaluator,EarlyStopping(patience=10),checkpoint,changelr
        ]

        self.model.fit_generator(
            data_gen_train(batch_size=batch_size,label_file=train_label,conv_shape=self.conv_shape),
            steps_per_epoch = steps_per_epoch,
            epochs=epochs,
            callbacks=callbacks,
            validation_data=data_gen_val(batch_size=batch_size,label_file=val_label,conv_shape=self.conv_shape),
            validation_steps=validation_steps
        )

        self.model.save('final_model.h5')

        print('[Model] Training Completed. Model saved as %s' % save_fname)
        timer.stop()
        
```

- SuperParam.py  参数
```
characters = '0123456789'
height = 85
width = 350
n_class = 11  # 36种字符
batch_size = 16
seq_len = 11 # 验证码长度 4self
n_len = 11  # 验证码长度 4self
no_random = 1
```

- DataLoad_Model.py 训练数据生成函数

```
    def gen_data_by_batch_ctc_hw(self,batch_size,label_file,conv_shape):
        '''
        生成ctc风格数据
        :param batch_size:
        :param height:
        :param width:
        :param n_class:
        :param n_len:
        :param characters:
        :return:
        '''

        # 获取文件清单 加载lebel dict
        if self.labels == {}:
            self.gen_label_dict_ctc(label_file)
        if self.FilePathList == []:
            self.get_path_list()
        #print(self.FilePathList)
        FilePathList = self.FilePathList
        FilePathList_Len = len(FilePathList)
        FileIndexs = np.random.randint(0, FilePathList_Len, batch_size)

        img_i = 0
        X = np.zeros((batch_size, height, width, 3), dtype=np.uint8)
        y = np.zeros((batch_size, n_len), dtype=np.uint8)

        for file_i in FileIndexs:
            if DEBUG:
                print(FilePathList[file_i])
            #file_name = os.path.basename(FilePathList[file_i]).split('.')[0]
            file_name = os.path.basename(FilePathList[file_i])
            img_y_labe = self.labels[file_name]
            ####################################
            #img_y_labe = img_y_labe[0:n_len]
            ####################################
            if img_y_labe == '':
                raise  Exception(file_name + ' not have label')
            img = cv2.imread(FilePathList[file_i])

            X[img_i, :] = cv2.resize(img, (width, height))
            y[img_i] = img_y_labe
            img_i = img_i + 1

        return [y,X,np.ones(batch_size) * int(conv_shape[1]),
                np.ones(batch_size)*n_len], np.ones(batch_size)
```                


#### 3.5 几点心得

> 1 GPU下完成模型收敛，10-15min CPU下12h以上（内存不足，减小batchsize）

> 2 bacthsize 超参数 一定情况影响收敛速度

> 3 如果发现loss下降困难，试试调整学习速率，会有惊喜

> 4 BN + POOLing 层 对模型的泛化效果十分明显

> 5 大神的模型就是不一样