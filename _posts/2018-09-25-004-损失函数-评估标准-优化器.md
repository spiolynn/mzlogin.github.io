---
layout:     post
title:      004-损失函数-评估标准-优化器
subtitle:    "\"004-损失函数-评估标准-优化器\""
date:       2018-09-25
author:     PZ
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - ML
    - 理论
    - Keras
---

## 004-损失函数-评估标准-优化器

[TOC]

### 1 keras中的loss函数


> https://zhuanlan.zhihu.com/p/34667893 <br> keras中的损失函数

> Keras 中的损失函数是（y_true,y_pred）的函数

#### 1 mean_squared_error 均方误差

```
def mean_squared_error(y_true, y_pred):
    return K.mean(K.square(y_pred - y_true), axis=-1)
```

**MSE**:  `$L= \frac {1}{n} \sum^n_{i=1} (y_{true}^{(i)} - y_{pred}^{(i)})^2$`


#### 2 mean_absolute_error 均绝对值误差

```
def mean_absolute_error(y_true, y_pred):
    return K.mean(K.abs(y_pred - y_true), axis=-1)
```

MAE：  `$L= \frac {1}{n} \sum^n_{i=1} |(y_{true}^{(i)} - y_{pred}^{(i)})|$`


#### 3 mean_absolute_percentage_error 

```
def mean_absolute_percentage_error(y_true, y_pred):
    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),
                                            K.epsilon(),
                                            None))
    return 100. * K.mean(diff, axis=-1)
```

MAPE：  `$L= \frac {1}{n} \sum^n_{i=1} |\frac {y_{true}^{(i)} - y_{pred}^{(i)}}{y_{true}^{(i)}}| \cdot 100$`


#### 4 mean_squared_logarithmic_error

```
def mean_squared_logarithmic_error(y_true, y_pred):
    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)
    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)
    return K.mean(K.square(first_log - second_log), axis=-1)
```

MAPE：  `$L= \frac {1}{n} \sum^n_{i=1} (log(y_{true}^{(i)} +1) - log( y_{pred}^{(i)}+1))^2$`


#### 5 squared_hinge

```
def squared_hinge(y_true, y_pred):
    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)), axis=-1)
```

`$L= \frac {1}{n} \sum^n_{i=1} (max(0,1-y_{pred}^{(i)} \cdot y_{true}^{(i)}))^2$`

#### 6 hinge

```
def hinge(y_true, y_pred):
    return K.mean(K.maximum(1. - y_true * y_pred, 0.), axis=-1)
```

`$L= \frac {1}{n} \sum^n_{i=1} max(0,1-y_{pred}^{(i)} \cdot y_{true}^{(i)})$`

#### 7 binary_crossentropy - 对数损失函数，log loss，与sigmoid相对应的损失函数

```
def binary_crossentropy(y_true, y_pred):
    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)
def binary_crossentropy(target, output, from_logits=False):
    """Binary crossentropy between an output tensor and a target tensor.

    # Arguments
        target: A tensor with the same shape as `output`.
        output: A tensor.
        from_logits: Whether `output` is expected to be a logits tensor.
            By default, we consider that `output`
            encodes a probability distribution.

    # Returns
        A tensor.
    """
    # Note: tf.nn.sigmoid_cross_entropy_with_logits
    # expects logits, Keras expects probabilities.
    if not from_logits:
        # transform back to logits
        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)
        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)
        output = tf.log(output / (1 - output))

    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,
                                                   logits=output)
```

`L(Y,P(Y|X)) = -logP(Y|X)`


### 2 自定义loss

#### 2.1 如何选择loss函数

> https://www.leiphone.com/news/201706/e0PuNeEzaXWsMPZX.html

### 3 评估标准 metrics

> 评价函数和 损失函数 相似，只不过评价函数的结果不会用于训练过程中。也是y_true,y_pred）的函数

### 4 自定义评价函数

> 自定义评价函数应该在编译的时候（compile）传递进去。该函数需要以 (y_true, y_pred) 作为输入参数，并返回一个张量作为输出结果。

```
import keras.backend as K

def mean_pred(y_true, y_pred):
    return K.mean(y_pred)

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy', mean_pred])
```

### 5 优化器

```
from keras import optimizers

# 所有参数梯度将被裁剪，让其l2范数最大为1：g * 1 / max(1, l2_norm)
sgd = optimizers.SGD(lr=0.01, clipnorm=1.)


from keras import optimizers

# 所有参数d 梯度将被裁剪到数值范围内：
# 最大值0.5
# 最小值-0.5
sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)

```

#### 1 SGD 随机梯度下降优化器

`keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)`

#### 2 RMSprop 

`keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)`

建议使用优化器的默认参数 （除了学习率lr，它可以被自由调节）

这个优化器通常是训练循环神经网络RNN的不错选择。


#### 3 Adagrad

`keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)`

> Adagrad是一种具有特定参数学习率的优化器，它根据参数在训练期间的更新频率进行自适应调整。 参数接收的更新越多，更新越小。

#### 4 Adadelta

`keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)`


> https://keras.io/zh/optimizers/

### 4 指标监控

> https://cloud.tencent.com/developer/article/1034630 <br> 使用Keras在训练深度学习模型时监控性能指标

> https://zhuanlan.zhihu.com/p/38080551 <br> KERAS 完成召回率、acc,recall,precision 评估值


### 5 自定义`metrics`

- 自定义`metrics`

```python
from keras.layers import Input, Dense
from keras.models import Model
import keras

from keras.callbacks import *

class Evaluate(Callback):
    def __init__(self):
        self.accs = []
    def on_epoch_begin(self, epoch, logs=None):
        print('every epoch ' + str(epoch) + 'begin')

    def on_epoch_end(self, epoch, logs=None):
        print('every epoch ' + str(epoch) + 'end')

    def on_batch_begin(self, batch, logs=None):
        print('every batch ' + str(batch) + 'begin')

    def on_batch_end(self, batch, logs=None):
        print('every batch ' + str(batch) + 'end')
        print(logs['loss'])
    def on_train_begin(self, logs=None):
        print('every train ' + 'begin')

    def on_train_end(self, logs=None):
        print('every train ' + 'end')

evaluator = Evaluate()


def mean_pred(y_true, y_pred):
    return K.mean(y_pred)

# 生成虚拟数据
import numpy as np
data = np.random.random((1000, 784))
## 生成0-9 随机数 (1000,1) 1000维列向量
labels = np.random.randint(10, size=(1000, 1))
# 将标签转换为分类的 one-hot 编码
one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)

# 这部分返回一个张量
inputs = Input(shape=(784,))

# 层的实例是可调用的，它以张量为参数，并且返回一个张量
x = Dense(64, activation='relu')(inputs)
x = Dense(64, activation='relu')(x)
predictions = Dense(10, activation='softmax')(x)

# 这部分创建了一个包含输入层和三个全连接层的模型
model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=[mean_pred])


model.fit(data, one_hot_labels,
          callbacks=[evaluator],validation_data=(data, one_hot_labels))  # 开始训练

```